#!/opt/conda/bin/python
from argparse import ArgumentParser
from collections import defaultdict
from dataclasses import dataclass
from multiprocessing import Process
from tqdm import tqdm
import os
import random
import heapq

os.chdir('/workspace/MSWC')
with open('languages') as f:
    languages = [line.strip() for line in f]
splits = {entry.name for entry in os.scandir('frequent')}

parser = ArgumentParser()
parser.add_argument('split', metavar='SPLIT', choices=splits)
parser.add_argument('-l', metavar='LANGUAGE', nargs='+',
        default=languages, choices=set(languages),
        dest='langs', help='languages to include')
parser.add_argument('-x', metavar='LANGUAGE', nargs='+', default=[],
        dest='exclude', help='languages to exclude')
parser.add_argument('-b', metavar='N', default=0, type=int,
        dest='baseline', help='num files lower bound')
parser.add_argument('-t', metavar='N', default=10**15, type=int,
        dest='threshold', help='num files upper bound')
parser.add_argument('-n', metavar='N', default=10**15, type=int,
        dest='max_words', help='num words to take')
parser.add_argument('--follow', action='store_true', help='take words from follow file')
parser.add_argument('--go', action='store_true', help='execute the preview')
args = parser.parse_args()

# REFINE ~ BASELINE, THRESHOLD
@dataclass
class Word:
    lang: str
    value: str
    freq: int

    def __str__(self):
        return f"{self.lang.replace('-', '_')}_{self.value}"

def refine(lang, args):
    FILE = f'frequent/{args.split}/{lang}.frequent'
    words = []
    with open(FILE) as f:
        for line in f:
            count, word = line.split()
            count = int(count)

            if count < args.baseline: continue
            count = min(count, args.threshold)

            word = Word(lang, word, count)
            words.append(word)

    return words

exclude = set(args.exclude)
langs = sorted(args.langs)

H = [refine(lang, args) for lang in langs if lang not in exclude]
H = list(heapq.merge(*H, reverse=True,
    key=lambda word: word.freq
))

if args.follow:
    with open('follows') as f:
        follows = {line.strip() for line in f}
    H = [word for word in H if str(word) in follows]

# CHOOSE WORDS
def choose(H, n):
    if n >= len(H): return H

    h = [H[n]]
    for l in range(n-1, -1, -1):
        if H[l].freq != H[n].freq: l+=1; break
        h.append(H[l])

    for r in range(n+1, len(H)):
        if H[r].freq != H[n].freq: r-=1; break
        h.append(H[r])

    random.seed(42)
    h = random.sample(h, n-l)
    return H[:l] + h

N = args.max_words
if N < 0:
    H.reverse()
    N = -N
H = choose(H, N)

# PREVIEW
if not args.go:
    lang_stat = defaultdict(lambda: {
        'num_words': 0,
        'num_files': 0
    })

    total_words, total_files = 0, 0
    for word in H:
        lang_stat[word.lang]['num_words'] += 1
        lang_stat[word.lang]['num_files'] += word.freq

        total_words += 1
        total_files += word.freq

    lang_stat['TOTAL']['num_words'] = total_words
    lang_stat['TOTAL']['num_files'] = total_files

    lang_stat = [(lang, *stat.values()) for lang, stat in lang_stat.items()]
    lang_stat.sort(key=lambda x: x[2])
    for lang, num_words, num_files in lang_stat:
        print(f'{lang:<15}{num_words:<10}{num_files}')
    exit(0)

# EXECUTE
@dataclass
class Extractor:
    split: str
    lang: str
    words: list

    def extend(self, link):
        return f"{self.lang}/clips/{link}"

    def get_lines(self):
        lang, split = self.lang, self.split
        idx = slice(1,3) if split == 'all' else slice(0,2)
        word_lines = {str(word) : [] for word in self.words}

        if split == 'all': split = 'splits'
        FILE = f'splits/{lang}/{lang}_{split}.csv'
        with open(FILE) as f:
            f.readline()
            for line in f:
                L = line.split(',')
                link, word = L[idx]

                word = str(Word(self.lang, word, 0))
                if word not in word_lines: continue
                link = self.extend(link)

                L[idx] = link, word
                line = ','.join(L)
                word_lines[word].append(line)

        return word_lines

    def extract(self):
        word_lines = self.get_lines()
        for word in self.words:
            lines = word_lines[str(word)]
            random.seed(42)
            lines = random.sample(lines, word.freq)

            print(*lines, sep='', end='')

lang_words = defaultdict(list)
for word in H: lang_words[word.lang].append(word)

extractors = [Extractor(args.split, lang, words)
    for lang, words in lang_words.items()]
ps = [Process(target=extractor.extract)
    for extractor in extractors]

header = 'LINK,WORD,VALID,SPEAKER,GENDER'
if args.split == 'all': header = f'SET,{header}'
print(header)

for p in ps: p.start()
for p in tqdm(ps): p.join()
